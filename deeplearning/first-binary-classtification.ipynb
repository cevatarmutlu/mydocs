{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Keras ile IMDB dataset\nKeras ile Imdb dataset' ini kullanarak yapılan binary classtification(ikili sınıflandırma) örneğidir. Imdb veri seti içinde `50000` adet filmin kritiğinin(sanırım film hakkında yapılan yorumlar) bulunduğu bir dataset. Film değerlendirmelerinden film hakkında olumlu veya olumsuz şekilde bir değerlendirme yapıyor. `Olumlu` ise `1` `olumsuz` ise `0`\n\nKaynak: Python ile Derin Öğrenme, Frainçois Chollet"},{"metadata":{},"cell_type":"markdown","source":"Imdb veriseti keras' ta default olarak geliyor yani aslında Imdb veriseti için bir yapı oluşturulmuş yoksa veri hazır gelmiyor.\n\nVeri seti aşağıdaki hücrede indiriliyor."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\n\n(train_data, train_labes), (test_data, test_labels) = imdb.load_data(num_words=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`load_data` Imdb verisetini indirip veriyi `list` dizileri şeklinde döndürüyor.\n\n`num_words` ise listelerin içinde `[0, 100000)` aralığının dışında veri bulunmasını önlüyor."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(\"Imdb verisetinin ilk elemanının görünümü:\\n\", train_data[0])\nprint(\"\\nİlk filmin kritiği: \", train_labes[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yukarıdaki hücre çıktısında her sayı bir kelimeyi temsil etmektedir. `Kelime: Sayı` değerlerinin bulunduğu bir JSON dosyası bulunmaktadır."},{"metadata":{"trusted":true},"cell_type":"code","source":"max([max(sequence) for sequence in train_data])\n# Her film kritiğinde [0, 10000) aralığında kelime seçtiğimiz için bu işlemin neticesi 10000' nı geçemez.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = imdb.get_word_index()\nprint(f\"\\n\\n'Kelime: Sayı' örneği: {list(word_index.keys())[0]}: {list(word_index.values())[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yukarıdaki hücre ile kelimelerin sayı karşılığını tutan `JSON` dosyasını indirip onu bir `dict` haline getirdi: `Kelime: Sayı`\n\nAşağıdaki hücre ile `Kelime: Sayı` formatındaki `dict`' i `Sayı: Kelime` formatına gönüştürdü."},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\nprint(f\"'Sayı: Kelime' örneği: {list(reverse_word_index.keys())[0]}: {list(reverse_word_index.values())[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aşağıdaki hücrede verileri hazırlıyoruz. Hazırlamaktan kasıt list olan verileri Numpy dizilerine çevirmek. Çünkü layer' lara list diziler vermek iyi bir fikir değil onun yerine tensor' lar göndermek iyi fikir.\n\nElimizdeki bir verileri iki şekilde tensor' a çevirebiliriz.\n   1. Bütün verileri eşit boyuta getirip eksik kalan kısımları 0' lar ile doldurmak\n   2. (verinin satır sayısı, kelime sayısı) şekilinde bir tensor oluşturup kelimelere karşılık gelen sayıların karışılığını 1 yapmak kalanlarını 0 yapmak. Mesela 560. kelime elimizde olsun. Oluşturduğumuz tensor' un gerekli satırının 560. index değerini 1 yapmak.\n\n`vectorize_sequences` yukarıda bahsedilen ikinci yolu yapmaktadır.\n\n`np.asarray().astype()` ile `label`' ları tutan dizileri numpy dizilerine çeviriyoruz ve `dtype` olarak `int` olan dizileri `float`' a çeviriyoruz."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results\n\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\ny_train = np.asanyarray(train_labes).astype('float32')\ny_test = np.asanyarray(test_labels).astype('float32')\n\nprint(f\"types: Data: {type(x_train)}, {type(y_train)}, Label: {type(x_test)}, {type(y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sinir ağımızı oluşturturduk\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000, )))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural Network Compile\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['acc']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validation veriseti oluşturmak\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=20,\n    batch_size=512,\n    validation_data=(x_val, y_val)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}